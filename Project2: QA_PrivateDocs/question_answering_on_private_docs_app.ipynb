{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_document(file_path):\n",
    "    \"\"\"\n",
    "    This function is used to load documents from different sources\n",
    "    :param file_path: The path to the file\n",
    "    :return: Returns the document's content\n",
    "    \"\"\"\n",
    "    loader = None\n",
    "    name, extension = os.path.splitext(file_path)\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file_path}')\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file_path}')\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    else:\n",
    "        print(f'The file extension of {extension} does not supported')\n",
    "    return loader.load()\n",
    "\n",
    "# Wikipedia\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    \"\"\"\n",
    "    This function is used to load documents from Wikipedia\n",
    "    :param query: is the text which is used to find docs\n",
    "    :param lang: is used to search in a specific language\n",
    "    :param load_max_docs: load_max_docs is to limit the number of downloaded docs.\n",
    "    :return: Returns a list of documents\n",
    "    \"\"\"\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    return loader.load()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1938249a8be86b0e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256) -> list:\n",
    "    \"\"\"\n",
    "    This function is used to chunk the data into smaller pieces\n",
    "    :param data: The data to be chunked\n",
    "    :param chunk_size: The size of the chunk\n",
    "    :return: Returns a list of chunks of the data\n",
    "    \"\"\"\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf2212efc4ef4640",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    \"\"\"\n",
    "    This function is used to print the embedding cost\n",
    "    :param texts: The texts to be embedded\n",
    "    :return: Returns the embedding cost\n",
    "    \"\"\"\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23311fa69d05a716",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Embedding and Uploading to a Vector Database (Pinecone)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efff685385a66b32"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name: str, chunks: list):\n",
    "    \"\"\"\n",
    "    This function is used to insert or fetch the embeddings from the vector database\n",
    "    :param index_name: The name of the index\n",
    "    :param chunks: The chunks to be inserted or fetched\n",
    "    :return: vector store\n",
    "    \"\"\"\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import PodSpec\n",
    "     \n",
    "    pc = pinecone.Pinecone()\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    \n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and use existing embeddings...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=PodSpec(environment='gcp-starter')\n",
    "        )\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
    "        print('Ok')\n",
    "    return vector_store"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60c932a99cfbd9c6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def delete_pinecone_index(index_name='all'):\n",
    "    \"\"\"\n",
    "    This function is used to delete the Pinecone index\n",
    "    :param index_name: The name of the index\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print(f'Deleting all indexes ....')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "            print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name}...', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cca4b765508bcf8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    \"\"\"\n",
    "    This function is used to ask a question and get an answer\n",
    "    :param vector_store: The vector store\n",
    "    :param q: The question\n",
    "    :return: Returns the answer\n",
    "    \"\"\"\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    return chain.invoke(q)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d5579b9b9dde58f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running Code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c42b764a35e6790e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_document('files/us_constitution.pdf')\n",
    "# print(data[1].page_content)\n",
    "# print(data[10].metadata)\n",
    "\n",
    "print(f'You have {len(data)} pages in your data')\n",
    "print(f'There are {len(data[20].page_content)} characters in the page')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b77e5004690cfb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chunks = chunk_data(data=data)\n",
    "print(len(chunks))\n",
    "# print(chunks[10].page_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad86d0e9a85359fd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(chunks[10].page_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5223d7b1d57dbab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print_embedding_cost(chunks)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21f5fcfecfc420cb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_document('files/the_great_gatsby.docx')\n",
    "print(data[0].page_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5089171411b48416",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_from_wikipedia(query=\"GPT-4\", lang='de')\n",
    "print(data[0].page_content)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f485c7a4c28e4e36",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Deleting all indexes  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a902ff715dc5fa2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "delete_pinecone_index()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b566aead7d620940",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create an index, and then create embeddings from document chunks and then uplaod both chunks and embeddings to Pinecone"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2094d2304ab91d65"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create an index on Pinecone\n",
    "index_name = 'your_index_name'\n",
    "vector_store = insert_or_fetch_embeddings(index_name=index_name, chunks=chunks)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20d3b5b482bee691",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q = 'What is the whole document about?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0a9368e3acd87c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Ask questions continuously.\n",
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i += 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting ... bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f'Question: {answer['query']}')\n",
    "    print(f'\\nAnswer: {answer['result']}')\n",
    "    print(f'\\n {\"-\" * 50} \\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd9c4657c3b9d5d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_from_wikipedia('ChatGPT', 'ro')\n",
    "chunks = chunk_data(data)\n",
    "index_name='your_index_name'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5b898adab2f6fa68",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q = \"Ce este ChatGPT?\"\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "617771f3bded541f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using Chroma as a Vector DB"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb943fac6a20fa27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pip install -q chromadb\n",
    "# If it is not installed, you can install it by running the following command\n",
    "# export HNSWLIB_NO_NATIVE=1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "375b6bbdca8be7b2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    \"\"\"\n",
    "    This function is used to create embeddings from document chunks and then upload both chunks and embeddings to Chroma and return the vector store object\n",
    "    :param chunks: The chunks to be inserted or fetched \n",
    "    :param persist_directory: The directory to persist the embeddings\n",
    "    :return: Returns vector store object\n",
    "    \"\"\"\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    return Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
    "\n",
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    \"\"\"\n",
    "    This function is used to load the existing embeddings from disk to a vector store object\n",
    "    :param persist_directory: The directory to persist the embeddings\n",
    "    :return: Returns a vector store object from an existing embeddings\n",
    "    \"\"\"\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
    "    return Chroma(persist_directory=persist_directory, embedding_function=embeddings)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47f8d76d3efcfa5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks=chunks)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee690f5b2a915392",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q = \"What is Vertex AI Search?\"\n",
    "answer = ask_and_get_answer(vector_store=vector_store, q=q)\n",
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26e03b9e912fe6d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "answer = ask_and_get_answer(vector_store=vector_store, q=q)\n",
    "print(answer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59462323eb4768ff",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Adding Memory (Chat History)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "499be59d3647c127"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# This chain is used to have a conversation based on the retrieved documents.\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# ConversationBufferMemory is for storing conversation into buffer\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
    "# A retriever is a crucial component that helps LLM find and access relevant information. Its aim is to search for relevant data and retrieve the information. The below code retrieves the top k most similar chunks of data. \n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "# `chat_history` is a label for the memory. It is used when interacting with the stored conversation.\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "# The below code is used to create a conversational retrieval chain. It is used to have a conversation based on the retrieved documents.\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff', # means, use all the text from documents\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "682663584161f8ef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ask_question(q, chain):\n",
    "    \"\"\"\n",
    "    This function is used to ask a question and get an answer\n",
    "    :param q: The question\n",
    "    :param chain: The chain\n",
    "    :return: Returns the answer\n",
    "    \"\"\"\n",
    "    return chain.invoke({'question': q})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9655d0604cbd577",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_document('files/rag_powered_by_google_search.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks=chunks)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c0b596a20455b1e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93ca99798f210570",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(result['answer'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "353bfff330b70e07",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Let's test if it remembers the last question."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "455b7ce99e7ac47d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q = 'Multiply that number by 10.'\n",
    "result = ask_question(q, crc)\n",
    "print(result)\n",
    "# It uses the previous answer arguments for the next answer by saving `chat_history`. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e51f8e988d9fc67",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Display the chat_history that contains all the questions and their answers, iterate over the content of the chat history key as follows\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d904e26b470b33d3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3229ecf62431c9ec",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using a Custom Prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33fcb1a660da256d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# This chain is used to have a conversation based on the retrieved documents.\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# ConversationBufferMemory is for storing conversation into buffer\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
    "# A retriever is a crucial component that helps LLM find and access relevant information. Its aim is to search for relevant data and retrieve the information. The below code retrieves the top k most similar chunks of data. \n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "# `chat_history` is a label for the memory. It is used when interacting with the stored conversation.\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "system_template = r'''\n",
    "Use the following pieces of context to answer the user's question.\n",
    "If you don't find the answer in the provided context, just respond \"I don't know.\"\n",
    "-------------------\n",
    "Context: ```{context}```\n",
    "'''\n",
    "\n",
    "user_template = r'''\n",
    "Question: ```{question}```\n",
    "Chat History: ```{chat_history}```\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "# Question-Answer Prompt\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff', # means, use all the text from documents\n",
    "    combine_docs_chain_kwargs={ 'prompt': qa_prompt },\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b3c2777dbef634a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(qa_prompt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a006be48bb6611d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "db = load_embeddings_chroma()\n",
    "q = 'When was Bill Gates born?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9261499e2c0f09b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q = 'How many pairs of questions and answers had the StackOverflow dataset?'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2751de66d6f03f26",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "q = 'Multiply that number by 10.'\n",
    "result = ask_question(q, crc)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c4f8a296265647",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3725664edc47867",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
